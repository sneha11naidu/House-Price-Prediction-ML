{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsKtwxgHO_92"
      },
      "source": [
        "# California housing Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIEbzpIc23Y8",
        "outputId": "1ef2972f-32af-4dab-dcc8-b5980bf57700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "housing_df = pd.read_csv('housing_coursework_entire_dataset_23-24.csv')\n",
        "# print(housing_df)\n",
        "print(type(housing_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiW3nbX739fS"
      },
      "source": [
        "# Data Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FsjguWUYagP"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cc9sKHVsYagQ"
      },
      "outputs": [],
      "source": [
        "# Common imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# random seed to make output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# data visualization\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler  # Make sure to import this\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4egMgFfF3S8"
      },
      "outputs": [],
      "source": [
        "# Drop the 'No.' column\n",
        "housing_df.drop('No.', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C41gSxmltDur"
      },
      "outputs": [],
      "source": [
        "housing_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aicS3xiytXVc"
      },
      "outputs": [],
      "source": [
        "housing_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apNQi9vK6gQ0"
      },
      "outputs": [],
      "source": [
        "#PREDICTIVE (INPUT) FEATURES AVAILABLE\n",
        "\n",
        "print(housing_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArAKlfh472tj"
      },
      "outputs": [],
      "source": [
        "print(housing_df['median_house_value'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r81W9rw38wqK"
      },
      "outputs": [],
      "source": [
        "#VIEW TOP 5 ROWS OF DATASET\n",
        "\n",
        "housing_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoTVpOgQ85Gw"
      },
      "source": [
        "# Data Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyhZxZzlPlX4"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "#CREATING A DISTRIBUTION PLOT FOR TARGET FEATURE - count of observations\n",
        "sns.displot(housing_df['median_house_value'], kde = True)\n",
        "plt.title('MedHouseVal Distribution')\n",
        "plt.xlabel(\"Median house value in ($100,000)\")\n",
        "plt.legend(['MedHouseVal Distribution'], loc = 'best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN5tx44hCqxv"
      },
      "outputs": [],
      "source": [
        "#VISUALIZING THE DISTRIBUTION IN SELECTED FEATURES EXLUDING MedHouseValues\n",
        "\n",
        "for feature in housing_df[:-1]:\n",
        "    sns.displot(housing_df[feature], kde = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFxFMC4nQMmU"
      },
      "outputs": [],
      "source": [
        "#HEATMAP TO VISUALIZE CORRELATION MATRIX\n",
        "sns.heatmap(housing_df.corr(), annot = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pmG4efRHrN-"
      },
      "outputs": [],
      "source": [
        "# Scatter Plots with Regression Lines\n",
        "\n",
        "sns.lmplot(x='median_income', y='median_house_value', data=housing_df, aspect=2, height=6)\n",
        "plt.xlabel('Median Income')\n",
        "plt.ylabel('Median House Value')\n",
        "plt.title('Scatter Plot with Regression Line: Median Income vs. Median House Value')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iSNMborYagZ"
      },
      "outputs": [],
      "source": [
        "housing_df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
        "    s=housing_df[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
        "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
        "    sharex=False)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOqjKBaeJmP2"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe0MruoDMCvG",
        "outputId": "ff6ac8a3-ed09-4de1-b364-86e6d42c7f40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-d915afad53eb>:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  corr_matrix = housing_df.corr()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "median_house_value    1.000000\n",
              "median_income         0.671420\n",
              "housing_median_age    0.106038\n",
              "total_rooms           0.093554\n",
              "households            0.033697\n",
              "total_bedrooms        0.013277\n",
              "population           -0.035675\n",
              "longitude            -0.042019\n",
              "latitude             -0.160713\n",
              "Name: median_house_value, dtype: float64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corr_matrix = housing_df.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVUdC37OQwA_",
        "outputId": "2c86b812-3312-4e57-8e32-d02e54f6dc37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "longitude             0\n",
              "latitude              0\n",
              "housing_median_age    0\n",
              "total_rooms           0\n",
              "total_bedrooms        9\n",
              "population            0\n",
              "households            0\n",
              "median_income         0\n",
              "median_house_value    0\n",
              "ocean_proximity       2\n",
              "dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#CHECK NUMBER OF MISSING VALUES IN EACH OF THE FEATURES (COLUMNS)\n",
        "housing_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWlYsQRTNnla"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Convert categorical column to numerical using one-hot encoding\n",
        "housing_df = pd.get_dummies(housing_df, columns=[\"ocean_proximity\"])\n",
        "\n",
        "# Drop the one-hot encoded categorical columns for imputation purposes\n",
        "numerical_features = housing_df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Create the imputer instance with a strategy to impute using the median\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "\n",
        "# Apply imputer to only the numerical features\n",
        "housing_df[numerical_features.columns] = imputer.fit_transform(numerical_features)\n",
        "\n",
        "# Continue with the rest of your code\n",
        "print(housing_df.head())\n",
        "\n",
        "\n",
        "# Find the indices where NaN values are located\n",
        "nan_indices = np.argwhere(np.isnan(housing_df.values))\n",
        "print(nan_indices)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RLXz80xEUQy"
      },
      "outputs": [],
      "source": [
        "# housing_df[\"total_bedrooms\"].fillna(housing_df[\"total_bedrooms\"].mean(), inplace=True)\n",
        "# housing_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owBJqT4LH12Y"
      },
      "outputs": [],
      "source": [
        "# # Assuming 'housing' is your DataFrame containing the dataset\n",
        "\n",
        "# # Convert 'ocean_proximity' column to categorical dtype\n",
        "# housing_df['ocean_proximity'] = housing_df['ocean_proximity'].astype('category')\n",
        "\n",
        "# # Perform one-hot encoding on the 'ocean_proximity' column\n",
        "# housing_df = pd.get_dummies(housing_df, columns=['ocean_proximity'], prefix='ocean')\n",
        "\n",
        "# # Display the updated DataFrame\n",
        "# print(housing_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_-5HBCAIApy",
        "outputId": "32b1e6f1-a2f5-46ec-a220-81fa36f7731f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "longitude                     0\n",
              "latitude                      0\n",
              "housing_median_age            0\n",
              "total_rooms                   0\n",
              "total_bedrooms                0\n",
              "population                    0\n",
              "households                    0\n",
              "median_income                 0\n",
              "median_house_value            0\n",
              "ocean_proximity_<1H OCEAN     0\n",
              "ocean_proximity_INLAND        0\n",
              "ocean_proximity_NEAR BAY      0\n",
              "ocean_proximity_NEAR OCEAN    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "housing_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTlk5mNnWlwH"
      },
      "source": [
        "# Seperating Target and Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZsi013MWrHo",
        "outputId": "564a0d18-dfab-4e85-8ea1-04854630c7d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
            "0      -122.12     37.70                17.0       2488.0           617.0   \n",
            "1      -122.21     38.10                36.0       3018.0           557.0   \n",
            "2      -122.22     38.11                43.0       1939.0           353.0   \n",
            "3      -122.20     37.78                52.0       2300.0           443.0   \n",
            "4      -122.19     37.79                50.0        954.0           217.0   \n",
            "..         ...       ...                 ...          ...             ...   \n",
            "995    -119.30     36.30                14.0       3023.0           469.0   \n",
            "996    -121.70     38.65                22.0       1360.0           282.0   \n",
            "997    -121.92     38.57                10.0       1320.0           246.0   \n",
            "998    -122.00     38.83                26.0        272.0            49.0   \n",
            "999    -121.21     39.49                18.0        697.0           150.0   \n",
            "\n",
            "     population  households  median_income  ocean_proximity_<1H OCEAN  \\\n",
            "0        1287.0       538.0         2.9922                        0.0   \n",
            "1        1445.0       556.0         3.8029                        0.0   \n",
            "2         968.0       392.0         3.1848                        0.0   \n",
            "3        1225.0       423.0         3.5398                        0.0   \n",
            "4         546.0       201.0         2.6667                        0.0   \n",
            "..          ...         ...            ...                        ...   \n",
            "995      1523.0       492.0         5.3602                        0.0   \n",
            "996       808.0       229.0         2.4167                        0.0   \n",
            "997       898.0       228.0         1.9327                        0.0   \n",
            "998       194.0        52.0         3.4187                        0.0   \n",
            "999       356.0       114.0         2.5568                        0.0   \n",
            "\n",
            "     ocean_proximity_INLAND  ocean_proximity_NEAR BAY  \\\n",
            "0                       0.0                       1.0   \n",
            "1                       0.0                       1.0   \n",
            "2                       0.0                       1.0   \n",
            "3                       0.0                       1.0   \n",
            "4                       0.0                       1.0   \n",
            "..                      ...                       ...   \n",
            "995                     1.0                       0.0   \n",
            "996                     1.0                       0.0   \n",
            "997                     1.0                       0.0   \n",
            "998                     1.0                       0.0   \n",
            "999                     1.0                       0.0   \n",
            "\n",
            "     ocean_proximity_NEAR OCEAN  \n",
            "0                           0.0  \n",
            "1                           0.0  \n",
            "2                           0.0  \n",
            "3                           0.0  \n",
            "4                           0.0  \n",
            "..                          ...  \n",
            "995                         0.0  \n",
            "996                         0.0  \n",
            "997                         0.0  \n",
            "998                         0.0  \n",
            "999                         0.0  \n",
            "\n",
            "[1000 rows x 12 columns]\n",
            "0      179900.0\n",
            "1      129900.0\n",
            "2      112700.0\n",
            "3      158400.0\n",
            "4      172800.0\n",
            "         ...   \n",
            "995    118600.0\n",
            "996    225000.0\n",
            "997    193800.0\n",
            "998     98400.0\n",
            "999     77100.0\n",
            "Name: median_house_value, Length: 1000, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "y_target = housing_df['median_house_value']\n",
        "X_features = housing_df.drop(['median_house_value'], axis = 1)\n",
        "print(X_features)\n",
        "print(y_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrDKNeWXabxW"
      },
      "source": [
        "# Target feature Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGhs4Donadjo"
      },
      "outputs": [],
      "source": [
        "# # In the EDA Phase, we saw that the distribution of our target variable is not a normal distribution but it is skewed which can affect the performance of many learning algorithms.\n",
        "\n",
        "# # So let's try to tranform our target distribution into a normal one. To do this we use a log transformation. We will use qq-plot to see the transformation effect.\n",
        "\n",
        "\n",
        "# #IMPORTING LIBRARIES TO PERFORM NORMALIZATION\n",
        "\n",
        "# from scipy.stats import norm\n",
        "# import scipy.stats as stats\n",
        "# import statsmodels.api as sm\n",
        "\n",
        "\n",
        "# # median_house_value BEFORE TRANSFORMATION\n",
        "\n",
        "# fig, ax = plt.subplots(1,2, figsize= (15,5))\n",
        "# fig.suptitle(\" qq-plot & distribution SalePrice \", fontsize= 15)\n",
        "\n",
        "# sm.qqplot(y_target, stats.t, distargs=(4,), fit = True, line = \"45\", ax = ax[0])\n",
        "\n",
        "# sns.distplot(y_target, kde = True, hist = True, fit = norm, ax = ax[1])\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZC-3doQayDw"
      },
      "outputs": [],
      "source": [
        "# # median_house_value after transformation\n",
        "\n",
        "# y_target_log = np.log1p(y_target)\n",
        "\n",
        "# fig, ax = plt.subplots(1,2, figsize= (15,5))\n",
        "# fig.suptitle(\"qq-plot & distribution SalePrice \", fontsize= 15)\n",
        "\n",
        "# sm.qqplot(y_target_log, stats.t, distargs=(4,), fit = True, line = \"45\", ax = ax[0])\n",
        "# sns.distplot(y_target_log, kde = True, hist = True, fit = norm, ax = ax[1])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4Q_CN7auLE7"
      },
      "source": [
        "# Splitting Dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOEHS-XeYagT",
        "outputId": "e65a3e44-f44d-4c00-db40-73ae7cbae3c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "800 Train Instances + 200 Test Instances\n"
          ]
        }
      ],
      "source": [
        "# TRAIN TEST SPLIT\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size = 0.2, random_state = 1)\n",
        "print(len(X_train), \"Train Instances +\", len(X_test), \"Test Instances\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_30_w2E8uTkp"
      },
      "source": [
        "# Feature Scaling (Feature Transformation)\n",
        "# Standard Scaling (Standardize the dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc3AOOtdubrK"
      },
      "outputs": [],
      "source": [
        "# #  We perform the scaling on the input features so that all the features have a comparable range, and the features with larger values don't become the only prominant feature in pridicting the value, and this alos helps the learing algorithms (gradient descent) to run faster.\n",
        "\n",
        "# #STANDARDIZE THE DATASET\n",
        "\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "\n",
        "# #SCALING THE TRAIN DATA - FIT AND TRANSFORM\n",
        "\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# #TRANSFORM (SCALE) TEST DATA USING SAME SCALER FITTED USED TRAIN SET\n",
        "\n",
        "# X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDuIBY0GyNO7",
        "outputId": "6eff49e4-1a51-43e4-d419-af987f1e24e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
            "Best Parameters: {'C': 100, 'epsilon': 0.2, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "Test RMSE: 75853.10047370296\n",
            "Test R-squared: 0.5063166010193023\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train, y_train, X_test, y_test are defined\n",
        "\n",
        "# Define the model and parameters for tuning\n",
        "svr = SVR()\n",
        "parameters = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['rbf', 'linear'],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'epsilon': [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV\n",
        "grid_search = GridSearchCV(svr, parameters, cv=3, scoring='r2', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit the model using grid search to find the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best SVR model\n",
        "best_svr = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the test set\n",
        "y_pred = best_svr.predict(X_test)\n",
        "\n",
        "# Calculate RMSE and R-squared for the test set\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print('Test RMSE:', rmse)\n",
        "print('Test R-squared:', r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V55heozq7hV8",
        "outputId": "b0905583-cd7e-44a7-805e-0aed950c5fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "Best max_depth: 15 with R-squared score: 0.4021365504743808\n",
            "Test RMSE: 71786.87057952608\n",
            "Test R-squared: 0.5578273339625693\n"
          ]
        }
      ],
      "source": [
        "# DecisionTreeRegressor\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train, y_train, X_test, y_test are your dataset splits\n",
        "\n",
        "# Define the model\n",
        "decision_tree_model = DecisionTreeRegressor(random_state=0)\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'max_depth': [15, 20, 25, 30, 35, 40]\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=decision_tree_model, param_grid=param_grid, cv=3, scoring='r2', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit the model using grid search to find the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Extract the best estimator (model with the best max_depth)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Print out the best max_depth\n",
        "print(f\"Best max_depth: {grid_search.best_params_['max_depth']} with R-squared score: {grid_search.best_score_}\")\n",
        "\n",
        "# Train the final model on the entire training set using the best hyperparameter\n",
        "# Note: This step is actually redundant here since grid_search.fit() already trains the best model\n",
        "# best_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the model on the test set\n",
        "y_pred_test = best_model.predict(X_test)\n",
        "\n",
        "# Calculate RMSE and R-squared for the test set\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "# Print performance metrics for the test set\n",
        "print(f\"Test RMSE: {rmse_test}\")\n",
        "print(f\"Test R-squared: {r2_test}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeJ8iyFw3vQb",
        "outputId": "fefd4c3d-87b1-4a9c-8b96-dbf0517949eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
            "Best parameters: {'poly__degree': 1}\n",
            "Best cross-validation R-squared score: 0.6356656759315557\n",
            "Test RMSE: 71095.20414748123\n",
            "Test R-squared: 0.5663069521174282\n"
          ]
        }
      ],
      "source": [
        "# LinearRegression\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Create a pipeline that includes polynomial feature transformation and linear regression\n",
        "pipeline = Pipeline([\n",
        "    ('poly', PolynomialFeatures()),\n",
        "    ('scaler', StandardScaler()), # Standardizing features\n",
        "    ('linear', LinearRegression())\n",
        "])\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'poly__degree': [1, 2, 3] # Trying out polynomial degrees 1 (linear), 2, and 3\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV object\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='r2', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit the model using grid search to find the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best combination\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation R-squared score: {grid_search.best_score_}\")\n",
        "\n",
        "# Using the best model found, make predictions on the test set\n",
        "y_pred_test = grid_search.predict(X_test)\n",
        "\n",
        "# Calculate RMSE and R-squared for the test set\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "# Print performance metrics for the test set\n",
        "print('Test RMSE:', rmse_test)\n",
        "print('Test R-squared:', r2_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0stKghS4Jqe",
        "outputId": "0a8ea0da-5e2e-4cc1-9fb6-65230b60d970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "Best alpha: 10 with R-squared score: 0.6364678507049707\n",
            "Test RMSE for Best Ridge Regression: 71148.83608471726\n",
            "Test R-squared for Best Ridge Regression: 0.5656523771197207\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train, y_train, X_test, y_test are your dataset splits\n",
        "\n",
        "# Define the model\n",
        "ridge_regression = Ridge(random_state=0)\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 1, 10, 100, 1000, 2500]  # Including your original values and some additional ones\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV object\n",
        "grid_search_ridge = GridSearchCV(estimator=ridge_regression, param_grid=param_grid, cv=3, scoring='r2', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit the model using grid search to find the best parameters\n",
        "grid_search_ridge.fit(X_train, y_train)\n",
        "\n",
        "# Best Ridge model\n",
        "best_ridge_model = grid_search_ridge.best_estimator_\n",
        "\n",
        "# Print out the best alpha\n",
        "print(f\"Best alpha: {grid_search_ridge.best_params_['alpha']} with R-squared score: {grid_search_ridge.best_score_}\")\n",
        "\n",
        "# Make predictions using the best model on the test set\n",
        "y_pred_test_best_ridge = best_ridge_model.predict(X_test)\n",
        "\n",
        "# Calculate RMSE and R-squared for the test set\n",
        "mse_test_best_ridge = mean_squared_error(y_test, y_pred_test_best_ridge)\n",
        "rmse_test_best_ridge = np.sqrt(mse_test_best_ridge)\n",
        "r2_test_best_ridge = r2_score(y_test, y_pred_test_best_ridge)\n",
        "\n",
        "# Print performance metrics for the test set\n",
        "print('Test RMSE for Best Ridge Regression:', rmse_test_best_ridge)\n",
        "print('Test R-squared for Best Ridge Regression:', r2_test_best_ridge)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tQA2rnO495W"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid = {\n",
        "    \"n_estimators\": [80, 100, 120]\n",
        "}\n",
        "\n",
        "# Create the RandomForestRegressor model\n",
        "random_forest_regressor = RandomForestRegressor(max_depth=25, random_state=42)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=random_forest_regressor, param_grid=param_grid, cv=5, scoring=\"r2\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and best parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Train the final model on the entire training set with best parameters\n",
        "final_model = RandomForestRegressor(n_estimators=best_params[\"n_estimators\"], max_depth=25, random_state=42)\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the test set\n",
        "y_pred_test = final_model.predict(X_test)\n",
        "\n",
        "# Calculate RMSE and R-squared for the test set\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "# Print performance metrics for the test set\n",
        "print('Test RMSE:', rmse_test)\n",
        "print('Test R-squared:', r2_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrY0ZKLPdzoW",
        "outputId": "30b19ae8-7747-4c8e-db02-3ca68b097756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 3647567828.1705236\n",
            "RMSE: 60395.09771637532\n",
            "Standard Deviation of Actual House Prices: 107956.56943071492\n",
            "R-squared: 0.6870281869560266\n"
          ]
        }
      ],
      "source": [
        "# Create the RandomForestRegressor model\n",
        "random_forest_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model using the training set\n",
        "random_forest_regressor.fit(X_train,y_train)\n",
        "\n",
        "# Make predictions using the test set\n",
        "y_pred = random_forest_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the mean squared error of the predictions\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Calculate the standard deviation of the actual house prices\n",
        "std_dev = np.std(y_test)\n",
        "\n",
        "print('RMSE:', rmse)\n",
        "print('Standard Deviation of Actual House Prices:', std_dev)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('R-squared:', r2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}